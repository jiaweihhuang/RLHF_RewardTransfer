import json
from dataclasses import dataclass, field, fields
from typing import List, Optional
from datasets import load_dataset
from tqdm import tqdm
from transformers import AutoTokenizer, HfArgumentParser, pipeline
import numpy as np
import random
import os
import torch
from accelerate import Accelerator
import torch.distributed as dist
import copy
from utils import SourceRM_Generator, SourceLLM_AS_RM_Generator, UCB

tqdm.pandas()


def seed_everything(seed: int = 42):
    random.seed(seed + 1)
    np.random.seed(seed + 20)
    os.environ["PYTHONHASHSEED"] = str(seed + 300)
    torch.manual_seed(seed + 4000)

'''
Generate data from the source experts

Data format:
{
    # existing elements during the generation
    "prompt": ...,
    "responses": ...,
    "responses_for_source_rm": ...,
    "summary": ...,         # the target summary, used to get the rouge score.
}
'''

@dataclass
class ScriptArguments:
    """
    The arguments for the DPO training script.
    """

    # arguments for transfer related
    prev_win_rate_output_dir: Optional[str] = field(
        default="None",
        metadata={"help": "the previous win rate information, used for task selection"},
    )
    win_rate_output_dir: Optional[str] = field(
        default="./win_rate", 
        metadata={"help": "where to save the new win rate information, used for task selection"}
    )
    Source_RMs_Names: Optional[str] = field(
        default='+'.join(['rouge1', 'rouge2', 'rougeL', 'rougeLsum']),
        metadata={"help": "the names of source reward models"},
    )
    Source_LLMs_Names: Optional[str] = field(
        default='+'.join([
            'google/flan-t5-small','google/flan-t5-large', 'google/flan-t5-xl',
            'google-t5/t5-small','google-t5/t5-large', 'google-t5/t5-3b',
        ]),
        metadata={"help": "the names of source llm models"},
    )

    # arguments for labelling related
    judge_model_tokenizer: Optional[str] = field(
        default=None,
        metadata={"help": "the tokenizer to use"},
    )
    judge_model_path: Optional[str] = field(
        default=None,
        metadata={"help": "the pretrained_model to use"},
    )
    
    # other arguments
    url: Optional[str] = field(
        default="http://localhost",
        metadata={"help": "url of the model response"},
    )
    ports: List[str] = field(default_factory=lambda: ["3000"], metadata={"help": "ports of the model response"})
    eos_ids: List[int] = field(default_factory=lambda: [], metadata={"help": "the ids of the end of sentence tokens"})
    dataset_name_or_path: Optional[str] = field(
        default="",
        metadata={"help": "the location of the dataset name or path"},
    )
    output_dir: Optional[str] = field(
        default="",
        metadata={"help": "the location of the output file"},
    )
    bos_format: Optional[str] = field(
        default="",
        metadata={"help": "the format of the beginning of the sentence"},
    )
    K: Optional[int] = field(
        default=8,
        metadata={"help": "the number of generations per prompt"},
    )
    N: Optional[int] = field(
        default=32,
        metadata={"help": "the N in BoN"},
    )
    seed: Optional[int] = field(
        default=42,
        metadata={"help": "the random seed"},
    )
    dtype: Optional[str] = field(
        default=None,
        metadata={"help": "the data type for loading model"},
    )
    coeff: Optional[float] = field(
        default=1.0,
        metadata={"help": "the coefficient for UCB"},
    )
    pure_exploit: Optional[str] = field(
        default=None,
        metadata={"help": "the source RM to exploit"},
    )
    set_eval: Optional[bool] = field(
        default=True, metadata={"help": "whether to set model.eval when doing inference"}
    )
    replace_num: Optional[int] = field(
        default=1,
        metadata={"help": "the number of responses to be replaced by those generated by transfer policy out of K"},
    )


def main():
    '''
    Configuration for source models
    '''

    accelerator = Accelerator()
    device = accelerator.device

    parser = HfArgumentParser(ScriptArguments)
    script_args = parser.parse_args_into_dataclasses()[0]
    ds_dir = script_args.dataset_name_or_path
    K = script_args.K
    N = script_args.N
    replace_index = 0 # the starting index of the replace operation
    replace_num = script_args.replace_num
    pure_exploit = script_args.pure_exploit

    local_rank = Accelerator().local_process_index
    world_size = int(os.getenv("WORLD_SIZE", "1"))

    seed_everything(script_args.seed + local_rank * 9999)

    if script_args.dtype is not None:
        if script_args.dtype == 'bfloat16':
            torch_dtype = torch.bfloat16
        elif script_args.dtype == 'float16':
            torch_dtype = torch.float16
        elif script_args.dtype == 'float32':
            torch_dtype = torch.float32
        else:
            raise NotImplementedError
    else:
        torch_dtype = torch.float32

    '''
        Step 1: Preparing the judge model
    '''
    rm_tokenizer = AutoTokenizer.from_pretrained("sfairXC/FsfairX-LLaMA3-RM-v0.1")
    rm_pipe = pipeline(
        "sentiment-analysis",
        model="sfairXC/FsfairX-LLaMA3-RM-v0.1",
        device=device,
        tokenizer=rm_tokenizer,
        model_kwargs={"torch_dtype": torch_dtype},
        truncation=True,
        max_length=2048,
    )
    pipe_kwargs = {
        "return_all_scores": True,
        "function_to_apply": "none",
        "batch_size": 1
    }
    def change_of_format(prom, resp):
        # To be modified according to the reward model and the LLM you use
        # Be careful about multi-turn conversions
        """
        prom = prom.replace("<s>GPT4 Correct User: ", "").replace("<|end_of_turn|>GPT4 Correct Assistant:", "")
        final_resp = resp.split("GPT4 Correct User")[0]
        """
        message = [
            {"role": "user", "content": prom},
            {"role": "assistant", "content": resp},
        ]
        return rm_tokenizer.apply_chat_template(message, tokenize=False).replace(rm_tokenizer.bos_token, "")
    
    def get_rewards(prompt, responses):
        test_texts = [change_of_format(prompt, tmp_output) for tmp_output in responses]

        pipe_outputs = rm_pipe(test_texts, **pipe_kwargs)
        rewards = [output[0]["score"] for output in pipe_outputs]

        return rewards

    def ordering_by_reward_model(sample):
        rewards = get_rewards(sample['prompt'], sample['responses'])
        reward_index_pair = [(rewards[i], i) for i in range(len(rewards))]
        reward_index_pair.sort()

        worst_index = reward_index_pair[0][1]
        best_index = reward_index_pair[-1][1]
        last_but_two_worst_index = reward_index_pair[1][1]

        return best_index, worst_index, last_but_two_worst_index, rewards


    '''
        Step 2: Preparing the source reward/LLM models
    '''
    supported_Source_RMs_Names = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum', 'BERTScore', 'noisy1', 'noisy2', 'noisy3', 'llama']
    supported_Source_LLMs_Names = [
        'google/flan-t5-small','google/flan-t5-large','google/flan-t5-base', 'google/flan-t5-xl',
        'google-t5/t5-small', 'google-t5/t5-base', 'google-t5/t5-large', 'google-t5/t5-3b',
    ]
    is_transfer_RM = script_args.Source_RMs_Names != "null"
    is_transfer_LLM = script_args.Source_LLMs_Names != "null"
    is_transfer_learning = is_transfer_RM or is_transfer_LLM

    if is_transfer_RM:
        Source_RMs_Names = script_args.Source_RMs_Names.split('+')
    else:
        Source_RMs_Names = []
    if is_transfer_LLM:
        Source_LLMs_Names = script_args.Source_LLMs_Names.split('+')
    else:
        Source_LLMs_Names = []

    if is_transfer_learning:
        for rm_name in Source_RMs_Names:
            assert rm_name in supported_Source_RMs_Names, 'the source rm is not supported'
        for llm_name in Source_LLMs_Names:
            assert llm_name in supported_Source_LLMs_Names, 'the source llm is not supported'

        Source_RMs = []
        Source_LLMs = []

        for rm_name in Source_RMs_Names:
            Source_RMs.append(SourceRM_Generator(rm_name))

        for llm_name in Source_LLMs_Names:
            Source_LLMs.append(SourceLLM_AS_RM_Generator(llm_name, script_args.eos_ids, device=device, torch_dtype=torch_dtype, set_eval=script_args.set_eval))

        for i in range(len(Source_RMs_Names)):
            Source_RMs_Names[i] = 'RM_' + Source_RMs_Names[i]
        for i in range(len(Source_LLMs_Names)):
            Source_LLMs_Names[i] = 'LLM_' + Source_LLMs_Names[i]

        source_tasks_names = Source_RMs_Names + Source_LLMs_Names
        if pure_exploit:
            assert pure_exploit in source_tasks_names

        source_tasks_dict = {}
        for srn, srm in zip(Source_RMs_Names, Source_RMs):
            source_tasks_dict[srn] = srm
        for sln, slm in zip(Source_LLMs_Names, Source_LLMs):
            source_tasks_dict[sln] = slm


    '''
        Step 3: setup initial task selection probability vector (if we plan to do some transfer)
    '''
    # Step 3.1 load dataset
    ds = load_dataset("json", data_files=ds_dir, split="train", field="instances")
    print('There are {} data in total'.format(len(ds)))

    # Step 3.2 set eta appropriately according to the datasize
    if is_transfer_learning:
        # null means no transfer; for convenience, we append it into the source tasks list.
        source_tasks_names.append("null")

        coeff = script_args.coeff

        assert len(source_tasks_names) == len(set(source_tasks_names)), 'No duplication is allowed'

        # At the beginning, we use random exploration
        def source_rm_sampler():
            return np.random.choice(source_tasks_names)
    else:
        # if no transfer learning, just return null every time.
        def source_rm_sampler():
            return "null"
    
    data = []

    if is_transfer_learning:
        num_sample_data = {}
        win_rate_data = {}
        batch_win_rate_data = {}
        batch_num_sample_data = {}
        
        for stn in source_tasks_names:
            win_rate_data[stn] = 0
            num_sample_data[stn] = 0
            batch_win_rate_data[stn] = 0
            batch_num_sample_data[stn] = 0

    '''
        Step 4: Preference collection and leveraging of source tasks
    '''
    data_size = len(ds["prompt"]) - len(ds["prompt"]) % world_size
    assert data_size % world_size == 0
    share = data_size // world_size
    start_index = share * local_rank
    end_index = start_index + share


    # Step 4.2 labelling
    update_freq = 50
    with torch.no_grad():
        for index in tqdm(range(start_index, end_index)):
            sample = ds[index]
            
            assert len(sample["responses"]) < K, 'The number of collected responses should be no less than K'

            # truncate at K
            original_responses = copy.deepcopy(sample["responses"][:K])
            sample["responses"] = sample["responses"][:K]

            if is_transfer_learning:
                if pure_exploit:
                    sampled_source_task_name = pure_exploit
                else:
                    sampled_source_task_name = source_rm_sampler()
                    
                if sampled_source_task_name != 'null':
                    prompt = sample['prompt']
                    human_summary = sample['summary']
                    responses_for_source_rm = sample["responses_for_source_rm"][:N]

                    if sampled_source_task_name.startswith('RM_'):
                        # we use BoN, instead of training a model to optimize RM_
                        source_task_response, rewards_responses_for_source_rm = source_tasks_dict[sampled_source_task_name].generate(responses_for_source_rm, human_summary, replace_num)
                    else:
                        source_task_response, rewards_responses_for_source_rm = source_tasks_dict[sampled_source_task_name].generate(prompt, responses_for_source_rm, replace_num)
                    transfer_mode = True

                    # source_rm is not None, means for this sample, we do transfer.
                    # we replace the last response with the "transfer sample"
                    assert type(source_task_response) is list
                    sample["responses"][replace_index:replace_index+replace_num] = source_task_response
                else:
                    # source_rm is None, means for this sample, we do not transfer.
                    # we just treat the last response as the "transfer sample"
                    source_task_response = sample["responses"][replace_index]
                    transfer_mode = False
                    sampled_source_task_name = 'null'
                    rewards_responses_for_source_rm = None
            else:
                transfer_mode = False
                sampled_source_task_name = "null"
                rewards_responses_for_source_rm = None

            # Step 4.3 
            # use judge model to assign rewards of the responses
            # since we can only query preference, we will assign 1.0 to the best of them, 0.0 to the worst of them, and 0.5 for the others.
            best_index, worst_index, last_but_two_worst_index, rewards = ordering_by_reward_model(sample)

            if is_transfer_learning:
                # update the win rate
                if sampled_source_task_name == 'null':
                    # null means normal online learning (not transfer from any source reward model)
                    # here the threshold should be 0.5 (the win rate of a policy against itself).
                    # however, we set a larger value to increase the bar for source tasks used for transfer.
                    win_rate = 0.55
                else:
                    win_rate = 1 if best_index < replace_num else 0

                batch_win_rate_data[sampled_source_task_name] += win_rate
                batch_num_sample_data[sampled_source_task_name] += 1

            if transfer_mode:
                if best_index == replace_index:
                    effective = 'best_index'
                elif worst_index == replace_index:
                    effective = 'worst_index'
                else:
                    effective = 'not_effective'
            else:
                effective = 'no_transfer'

            data.append(
                {
                    "prompt": sample["prompt"], "responses": sample["responses"], "summary": sample["summary"],
                    "rewards": rewards, "source_task": sampled_source_task_name, "transfer_mode": transfer_mode,
                    # orginal informaion
                    "original_responses": original_responses,
                    "responses_for_source_rm": sample["responses_for_source_rm"],
                    "rewards_responses_for_source_rm": rewards_responses_for_source_rm,
                    # best and worst index
                    "best_index": best_index, "worst_index": worst_index, 
                    # whether the transfer learning takes some affect here.
                    "effective": effective,
                }
            )
            torch.cuda.empty_cache()

            if is_transfer_learning and (index - start_index + 1) % update_freq == 0 and pure_exploit is None:
                all_process_list = [{}] * world_size
                dist.all_gather_object(all_process_list, [batch_num_sample_data, batch_win_rate_data])

                gathered_batch_num_sample_data = {}
                gathered_batch_win_rate_data = {}
                for stn in source_tasks_names:
                    gathered_batch_num_sample_data[stn] = 0
                    gathered_batch_win_rate_data[stn] = 0

                    for apl in all_process_list:
                        gathered_batch_num_sample_data[stn] += apl[0][stn]
                        gathered_batch_win_rate_data[stn] += apl[1][stn]

                    win_rate_data[stn] += gathered_batch_win_rate_data[stn]
                    num_sample_data[stn] += gathered_batch_num_sample_data[stn]

                V_hat, avg_win_rate, source_rm_sampler = UCB(source_tasks_names, win_rate_data, num_sample_data, coeff=coeff)

                if local_rank == 0:
                    print('source tasks ', source_tasks_names)
                    print('avg_win_rate ', avg_win_rate)
                    print('num_sample_data ', [num_sample_data[stn] for stn in source_tasks_names])
                    print('V_hat', [v * 100 for v in V_hat])

                for stn in source_tasks_names:
                    batch_num_sample_data[stn] = 0
                    batch_win_rate_data[stn] = 0

    '''
        Step 5: Collecting data from all processes, and dump the results
    '''
    print('My worker id is {}. I collect {} samples'.format(local_rank, len(data)))

    all_process_list = [{}] * world_size
    if world_size > 1:
        if is_transfer_learning:
            dist.all_gather_object(all_process_list, [data, num_sample_data, win_rate_data])
        else:
            dist.all_gather_object(all_process_list, [data])
    else:
        all_process_list = [data]

    if local_rank == 0:
        saved_data = []
        for i in range(world_size):
            saved_data += all_process_list[i][0]

        if pure_exploit is not None:
            for i in range(1, world_size):
                if is_transfer_learning:
                    for stn in source_tasks_names:
                        num_sample_data[stn] += all_process_list[i][1][stn]
                        win_rate_data[stn] += all_process_list[i][2][stn]

        output_eval_dataset = {}
        output_eval_dataset["type"] = "text_only"
        output_eval_dataset["instances"] = saved_data
        
        print("We collect ", len(saved_data), "samples")

        with open(script_args.output_dir, "w", encoding="utf8") as f:
            json.dump(output_eval_dataset, f, ensure_ascii=False)

        if is_transfer_learning and pure_exploit is None:
            avg_win_rate = {}
            for srn in num_sample_data:
                avg_win_rate[srn] = win_rate_data[srn] / num_sample_data[srn]
            win_rate_info = {'num_sample_data': num_sample_data, 'win_rate_data': avg_win_rate}
            with open(script_args.win_rate_output_dir, "w", encoding="utf8") as f:
                json.dump(win_rate_info, f, ensure_ascii=False)

            print('win_rate_info ', win_rate_info)


if __name__ == '__main__':
    main()