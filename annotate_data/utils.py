import torch
import evaluate
import numpy as np
from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch.nn.functional as F

'''
We support two class of source tasks
(1) source RM: the suggested response will be generated by BoN. (i.e. provide multiple responses and return the one with the highest reward) 
(2) source LLM: the suggested response will be sampled by inferring the LLM (just treat log \pi as the source reward model)
'''
class SourceRM_Generator:
    def __init__(self, source_rm):
        self.source_rm = source_rm
        if 'rouge' in source_rm:
            self.rouge = evaluate.load('rouge')
            self.reward_func = lambda predictions, references: self.rouge.compute(predictions=[predictions], references=[references], rouge_types=[source_rm])[source_rm].item()
        elif source_rm == 'BERTScore':
            self.bertscore = evaluate.load("bertscore")
            self.reward_func = lambda predictions, references: self.bertscore.compute(predictions=[predictions], references=[references], model_type="distilbert-base-uncased")['f1'][0]
        else:
            raise NotImplementedError

    def generate(self, responses, human_summary, num_samples=1):
        rewards = []
        for r in responses:
            rewards.append(self.reward_func(r, human_summary))
        indicies = np.argsort(rewards)[::-1][:num_samples]
        selected_responses = []
        for ind in indicies:
            selected_responses.append(responses[ind])
        return selected_responses, rewards
    
    
'''
Query the log_prob of the LLM and use it as the reward function for selection
'''
class SourceLLM_AS_RM_Generator:
    def __init__(self, model_name, eos_ids, device, torch_dtype=torch.bfloat16, set_eval=True, ref_model=None):
        self.device = device
        # load the model
        self.tokenizer = T5Tokenizer.from_pretrained(model_name, device_map='cuda', torch_dtype=torch_dtype)
        self.model = T5ForConditionalGeneration.from_pretrained(model_name, device_map='cuda', torch_dtype=torch_dtype)
        self.eos_token_id = [self.tokenizer.eos_token_id] + eos_ids

        if ref_model:
            self.ref_model_tokenizer = T5Tokenizer.from_pretrained(ref_model, device_map='cuda', torch_dtype=torch_dtype)
            self.ref_model = T5ForConditionalGeneration.from_pretrained(ref_model, device_map='cuda', torch_dtype=torch_dtype)
        else:
            self.ref_model_tokenizer = None
            self.ref_model = None

        if set_eval:
            self.model.eval()
            
    def generate(self, prompt, responses, num_samples=1):
        log_probs = []

        tokenized_prompt = self.tokenizer(prompt, truncation=True, padding='longest', max_length=1000, return_tensors='pt')
        source_ids = tokenized_prompt['input_ids'].to(self.device)
        source_mask = tokenized_prompt['attention_mask'].to(self.device)

        for r in responses:
            lp = compute_log_prob_with_raw_texts(self.model, self.tokenizer, (source_ids, source_mask), r, self.device)
            log_probs.append(lp[0])

        if self.ref_model:
            ref_log_probs = []
            for r in responses:
                ref_lp = compute_log_prob_with_raw_texts(self.ref_model, self.ref_model_tokenizer, (source_ids, source_mask), r, self.device)
                ref_log_probs.append(ref_lp[0])
            rewards = [lp - rlp for (lp, rlp) in zip(log_probs, ref_log_probs)]
        else:
            rewards = log_probs

        indicies = np.argsort(rewards)[::-1][:num_samples]
        
        selected_responses = []
        for ind in indicies:
            selected_responses.append(responses[ind])
        return selected_responses, rewards
    

def compute_log_prob_with_raw_texts(model, tokenizer, prompt, response, device):
    if type(prompt) is tuple:
        source_ids, source_mask = prompt
    else:
        tokenized_prompt = tokenizer(prompt, truncation=True, padding='longest', max_length=1000, return_tensors='pt')
        source_ids = tokenized_prompt['input_ids'].to(device)
        source_mask = tokenized_prompt['attention_mask'].to(device)
    tokenized_response = tokenizer(response, truncation=True, padding='longest', max_length=1000, return_tensors='pt')
    target_ids = tokenized_response['input_ids'].to(device)
    output = model(
        input_ids=source_ids,
        attention_mask=source_mask,
        labels=target_ids,
        return_dict=True,
    )
    masked_logits = output['logits']
    normalized_logits = F.log_softmax(masked_logits, dim=-1)
    scores = normalized_logits.gather(-1, target_ids.unsqueeze(-1)).squeeze()
    is_not_padding = (target_ids > 0.0).float()
    masked_scores = scores * is_not_padding
    log_probs = masked_scores.sum(1).detach().cpu().tolist()
    return log_probs


def UCB(source_rm_names, win_rate_data, num_sample_data, coeff=1.0):
    win_rate = np.array([win_rate_data[srn] for srn in source_rm_names])
    N = np.array([num_sample_data[srn] for srn in source_rm_names]) + 1e-6      # small constant to avoid nan

    avg_win_rate = win_rate / N
    bonus = coeff / np.sqrt(N)

    V_hat = avg_win_rate + bonus

    index = np.argmax(V_hat)

    return V_hat, avg_win_rate, lambda: source_rm_names[index], 